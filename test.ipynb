{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pyspark_parquet_example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = f'data/sparkify/song-data/*/*/*/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"yarn\") \\\n",
    "        .appName('dataproc-udacity') \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to read the song data in the\n",
    "        filepath (bucket/song_data) to get the song and artist info and\n",
    "        used to populate the songs and artists dim tables.\n",
    "    Parameters:\n",
    "        spark: the cursor object.\n",
    "        input_path: the path to the bucket containing song data.\n",
    "        output_path: the path to destination bucket where the parquet files\n",
    "            will be stored.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = f'{input_data}/song-data/*/*/*/*.json'\n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "    print('Read song_data from GCS')\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select('song_id', 'title', 'artist_id',\n",
    "                            'year', 'duration').dropDuplicates()\n",
    "\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.parquet(f'{output_data}/songs_table',\n",
    "                              mode='overwrite',\n",
    "                              partitionBy=['year', 'artist_id'])\n",
    "    print('Writing songs_table to parquet')\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name',\n",
    "                              'artist_location', 'artist_latitude',\n",
    "                              'artist_longitude').dropDuplicates()\n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(f'{output_data}/artists_table',\n",
    "                                mode='overwrite')\n",
    "    print('Writting artists_table to parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = 'data/sparkify'\n",
    "output_data = 'data/sparkify'\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read song_data from GCS\n",
      "Writing songs_table to parquet\n",
      "Writting artists_table to parquet\n"
     ]
    }
   ],
   "source": [
    "process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = f'{input_data}/log-data/*.json'\n",
    "df = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harmonia</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>655.77751</td>\n",
       "      <td>free</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.541017e+12</td>\n",
       "      <td>583</td>\n",
       "      <td>Sehr kosmisch</td>\n",
       "      <td>200</td>\n",
       "      <td>1542241826796</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Prodigy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>Smith</td>\n",
       "      <td>260.07465</td>\n",
       "      <td>free</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.541017e+12</td>\n",
       "      <td>583</td>\n",
       "      <td>The Big Gundown</td>\n",
       "      <td>200</td>\n",
       "      <td>1542242481796</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>Smith</td>\n",
       "      <td>205.45261</td>\n",
       "      <td>free</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.541017e+12</td>\n",
       "      <td>583</td>\n",
       "      <td>Marry Me</td>\n",
       "      <td>200</td>\n",
       "      <td>1542242741796</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Wyatt</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Scott</td>\n",
       "      <td>NaN</td>\n",
       "      <td>free</td>\n",
       "      <td>Eureka-Arcata-Fortuna, CA</td>\n",
       "      <td>GET</td>\n",
       "      <td>Home</td>\n",
       "      <td>1.540872e+12</td>\n",
       "      <td>563</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1542247071796</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Austin</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Rosales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>free</td>\n",
       "      <td>New York-Newark-Jersey City, NY-NJ-PA</td>\n",
       "      <td>GET</td>\n",
       "      <td>Home</td>\n",
       "      <td>1.541060e+12</td>\n",
       "      <td>521</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1542252577796</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; rv:31.0) Gecko/20...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist       auth firstName gender  itemInSession lastName     length  \\\n",
       "0     Harmonia  Logged In      Ryan      M              0    Smith  655.77751   \n",
       "1  The Prodigy  Logged In      Ryan      M              1    Smith  260.07465   \n",
       "2        Train  Logged In      Ryan      M              2    Smith  205.45261   \n",
       "3         None  Logged In     Wyatt      M              0    Scott        NaN   \n",
       "4         None  Logged In    Austin      M              0  Rosales        NaN   \n",
       "\n",
       "  level                               location method      page  registration  \\\n",
       "0  free     San Jose-Sunnyvale-Santa Clara, CA    PUT  NextSong  1.541017e+12   \n",
       "1  free     San Jose-Sunnyvale-Santa Clara, CA    PUT  NextSong  1.541017e+12   \n",
       "2  free     San Jose-Sunnyvale-Santa Clara, CA    PUT  NextSong  1.541017e+12   \n",
       "3  free              Eureka-Arcata-Fortuna, CA    GET      Home  1.540872e+12   \n",
       "4  free  New York-Newark-Jersey City, NY-NJ-PA    GET      Home  1.541060e+12   \n",
       "\n",
       "   sessionId             song  status             ts  \\\n",
       "0        583    Sehr kosmisch     200  1542241826796   \n",
       "1        583  The Big Gundown     200  1542242481796   \n",
       "2        583         Marry Me     200  1542242741796   \n",
       "3        563             None     200  1542247071796   \n",
       "4        521             None     200  1542252577796   \n",
       "\n",
       "                                           userAgent userId  \n",
       "0  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...     26  \n",
       "1  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...     26  \n",
       "2  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...     26  \n",
       "3  Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....      9  \n",
       "4  Mozilla/5.0 (Windows NT 6.1; rv:31.0) Gecko/20...     12  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: This function can be used to read the log data in the\n",
    "        filepath (bucket/log_data) to get the info to populate the\n",
    "        user, time and song dim tables as well as the songplays fact table.\n",
    "    Parameters:\n",
    "        spark: the cursor object.\n",
    "        input_path: the path to the bucket containing song data.\n",
    "        output_path: the path to destination bucket where the parquet files\n",
    "            will be stored.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = f'{input_data}/log-data/*.json'\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    print('Reading log_data from GCS')\n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df['page'] == 'NextSong')\n",
    "\n",
    "    # extract columns for users table\n",
    "    user_table = df.select('userId', 'firstName', 'lastName',\n",
    "                           'gender', 'level').dropDuplicates()\n",
    "    # write users table to parquet files\n",
    "    user_table.write.parquet(f'{output_data}/user_table', mode='overwrite')\n",
    "    print('Writing user_table to parquet')\n",
    "    # create timestamp column from original timestamp column\n",
    "    df = df.withColumn('start_time', Function.from_unixtime(Function.col('ts')/1000))\n",
    "    print('Converting ts to timestamp')\n",
    "    # create datetime column from original timestamp column\n",
    "    time_table = df.select('ts', 'start_time') \\\n",
    "                   .withColumn('year', Function.year('start_time')) \\\n",
    "                   .withColumn('month', Function.month('start_time')) \\\n",
    "                   .withColumn('week', Function.weekofyear('start_time')) \\\n",
    "                   .withColumn('weekday', Function.dayofweek('start_time')) \\\n",
    "                   .withColumn('day', Function.dayofyear('start_time')) \\\n",
    "                   .withColumn('hour', Function.hour('start_time')).dropDuplicates()\n",
    "    print('Extract DateTime Columns')\n",
    "\n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.parquet(f'{output_data}/time_table',\n",
    "                             mode='overwrite',\n",
    "                             partitionBy=['year', 'month'])\n",
    "    print('Write time_table to parquet')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_data = f'{input_data}/song-data/A/A/A/*.json'\n",
    "    song_dataset = spark.read.json(song_data)\n",
    "    print('Read song_dataset from GCS')\n",
    "    # join & extract cols from song and log datasets to create songplays table\n",
    "    song_dataset.createOrReplaceTempView('song_dataset')\n",
    "    time_table.createOrReplaceTempView('time_table')\n",
    "    df.createOrReplaceTempView('log_dataset')\n",
    "    songplays_table = spark.sql(\"\"\"SELECT DISTINCT\n",
    "                                       l.ts as ts,\n",
    "                                       t.year as year,\n",
    "                                       t.month as month,\n",
    "                                       l.userId as user_id,\n",
    "                                       l.level as level,\n",
    "                                       s.song_id as song_id,\n",
    "                                       s.artist_id as artist_id,\n",
    "                                       l.sessionId as session_id,\n",
    "                                       s.artist_location as artist_location,\n",
    "                                       l.userAgent as user_agent\n",
    "                                   FROM song_dataset s\n",
    "                                   JOIN log_dataset l\n",
    "                                       ON s.artist_name = l.artist\n",
    "                                       AND s.title = l.song\n",
    "                                       AND s.duration = l.length\n",
    "                                   JOIN time_table t\n",
    "                                       ON t.ts = l.ts\n",
    "                                   \"\"\").dropDuplicates()\n",
    "    print('SQL Query')\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet(f'{output_data}/songplays_table',\n",
    "                                  mode='overwrite',\n",
    "                                  partitionBy=['year', 'month'])\n",
    "    print('Writing songplays_table to parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading log_data from GCS\n",
      "Writing user_table to parquet\n",
      "Converting ts to timestamp\n",
      "Extract DateTime Columns\n",
      "Write time_table to parquet\n",
      "Read song_dataset from GCS\n",
      "SQL Query\n",
      "Writing songplays_table to parquet\n"
     ]
    }
   ],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
